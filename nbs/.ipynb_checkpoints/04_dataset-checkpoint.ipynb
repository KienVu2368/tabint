{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class\n",
    "> convert data into tabular dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tabint.utils import *\n",
    "from tabint.pre_processing import *\n",
    "from tabint.transform import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.validation import _num_samples, check_array\n",
    "from sklearn.model_selection._split import _approximate_mode, _validate_shuffle_split\n",
    "from sklearn.utils import indexable, check_random_state, safe_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TBDataset:\n",
    "    \"\"\"\n",
    "    Contain train, validation, test set\n",
    "    \"\"\"\n",
    "    def __init__(self, x_trn, x_val, x_tst, x_tfms, y_trn, y_val, y_tfms):\n",
    "        self.x_trn, self.y_trn, self.x_tst = x_trn, y_trn, x_tst\n",
    "        self.x_val, self.y_val = x_val, y_val\n",
    "        self.x_tfms, self.y_tfms = x_tfms, y_tfms\n",
    "\n",
    "    @classmethod\n",
    "    def from_Split(cls, df, y = None, y_field = None, tp = '_',\n",
    "                    cats = None, x_tst = None, time_feature = None, ratio = 0.2,\n",
    "                     x_tfms = None, y_tfms = None, **kargs):\n",
    "        \"\"\"\n",
    "        use sklearn split function to split data\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        if y is None: y = df[y_field]; df = df.drop(y_field, axis = 1)\n",
    "        if x_tfms is None: x_tfms = noop_transform; x_tfms.transform(df)\n",
    "        if cats is None: cats = x_tfms.cats\n",
    "        if tp != 'time series': x_trn, y_trn, x_val, y_val = stratify_split(df, y, cats, ratio)\n",
    "        else: x_trn, y_trn, x_val, y_val = split_time_series(df, time_feature, ratio)\n",
    "\n",
    "        x_trn, x_val, x_tst, y_trn, y_val, x_tfms, y_tfms = cls.transform_data(x_trn, x_val, x_tst, y_trn, y_val, x_tfms, y_tfms)\n",
    "\n",
    "        return cls(x_trn, x_val, x_tst, x_tfms, y_trn, y_val, y_tfms)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_data(x_trn, x_val, x_tst, y_trn, y_val, x_tfms, y_tfms):\n",
    "        x_trn = x_tfms.transform(x_trn)\n",
    "        x_val = x_tfms.transform(x_val)\n",
    "        if x_tst is not None: x_tst = x_tfms.transform(x_tst)\n",
    "\n",
    "        if y_tfms is None: y_tfms = noop_transform\n",
    "        y_tfms.fit(y_trn)\n",
    "        y_trn = y_tfms.transform(y_trn)\n",
    "        y_val = y_tfms.transform(y_val)\n",
    "\n",
    "        return x_trn, x_val, x_tst, y_trn, y_val, x_tfms, y_tfms\n",
    "\n",
    "    def val_permutation(self, features):\n",
    "        \"\"\"\"\n",
    "        permute one or many columns of validation set. For permutation importance\n",
    "        \"\"\"\n",
    "        features = to_iter(features)\n",
    "        df = self.x_val.copy()\n",
    "        for ft in features: df[ft] = np.random.permutation(df[ft])\n",
    "        return df\n",
    "\n",
    "    def apply_function(self, feature, function_dict, inplace = True, tp = 'trn'):\n",
    "        \"\"\"\n",
    "        apply a function f for all dataset\n",
    "        \"\"\"\n",
    "        features = to_iter(features)\n",
    "        step = apply_function(function_dict).fit(self.x_trn)\n",
    "        self.apply_step(step, features, inplace, tp)\n",
    "\n",
    "    def sample(self, tp = 'trn', ratio = 0.3):\n",
    "        \"\"\"\n",
    "        get sample of dataset\n",
    "        \"\"\"\n",
    "        if 'tst' == tp:\n",
    "            return None if self.x_tst is None else self.x_tst.sample(self.x_tst.shape[0]*ratio)\n",
    "        else:\n",
    "            df, y = (self.x_trn, self.y_trn) if tp == 'trn' else (self.x_val, self.y_val)\n",
    "            _, df, _, y = train_test_split(df, y, test_size = ratio, stratify = y)\n",
    "            return df, y\n",
    "\n",
    "    def select(self, features, inplace = True, tp = 'trn'):\n",
    "        \"\"\"\n",
    "        keep columns of dataset\n",
    "        \"\"\"\n",
    "        features = to_iter(features)\n",
    "        step = select(features).fit(self.x_trn)\n",
    "        self.apply_step(step, features, inplace, tp)\n",
    "\n",
    "    def drop(self, feature, inplace = True, tp = 'trn'):\n",
    "        \"\"\"\n",
    "        drop columns of dataset\n",
    "        \"\"\"\n",
    "        features = to_iter(features)\n",
    "        step = drop_features(features).fit(self.x_trn)\n",
    "        self.apply_step(step, features, inplace, tp)\n",
    "\n",
    "    def remove_outlier(self, features = None, inplace = True, tp = 'trn', return_mask = False):\n",
    "        features = features or self.cons\n",
    "        features = to_iter(features)\n",
    "        mask_trn = self.get_mask_outlier(self.x_trn, features)\n",
    "        mask_val = self.get_mask_outlier(self.x_val, features)\n",
    "        if inplace:\n",
    "            self.x_trn, self.y_trn = self.x_trn[mask_trn], self.y_trn[mask_trn]\n",
    "            self.x_val, self.y_val = self.x_val[mask_val], self.y_val[mask_val]\n",
    "        else:\n",
    "            res = [self.x_trn[mask_trn], self.y_trn[mask_trn]] if tp == 'trn' else [self.x_val[mask_val], self.y_val[mask_val]]\n",
    "            if return_mask: res.append(mask_trn if tp == 'trn' else mask_val)\n",
    "            return res\n",
    "\n",
    "    def get_mask_outlier(self, df, features):\n",
    "        step = remove_outlier(features)\n",
    "        step.fit(df)\n",
    "        _ = step.transform(df)\n",
    "        mask = step.mask\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def apply_step(self, step, features, inplace, tp):\n",
    "        if inplace:\n",
    "            x_tfms.append(step)\n",
    "            self.x_trn = step.transform(self.x_trn)\n",
    "            self.x_val = step.transform(self.x_val)\n",
    "            if self.x_tst is not None: self.x_tst = step.transform(self.x_tst)\n",
    "            x_tfms.get_features(self.x_trn)\n",
    "        else:\n",
    "            if tp == 'tst': return None if self.x_tst is None else step.transform(self.x_tst)\n",
    "            else: return (step.transform(self.x_trn), self.y_trn) if tp == 'trn' else (step.transform(self.x_val), self.y_val)\n",
    "\n",
    "    @property\n",
    "    def cons(self): return self.x_tfms.cons\n",
    "\n",
    "    @property\n",
    "    def cats(self): return self.x_tfms.cats\n",
    "\n",
    "    @property\n",
    "    def features(self): return self.x_trn.columns\n",
    "\n",
    "    @property\n",
    "    def trn(self): return self.x_trn, self.y_trn\n",
    "\n",
    "    @property\n",
    "    def n_trn(self): return self.x_trn.shape[0]\n",
    "\n",
    "    @property\n",
    "    def val(self): return self.x_val, self.y_val\n",
    "\n",
    "    @property\n",
    "    def n_val(self): return self.x_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def random_choose(x, pct = 0.1, ratio = 0.2, **kargs):\n",
    "    \"\"\"\n",
    "    static method for from_TBSplit, random choose rows from a group\n",
    "    \"\"\"\n",
    "    n = x.shape[0] if random.uniform(0,1) <= pct else int(np.round(x.shape[0]*(ratio-0.04)))\n",
    "    return x.sample(n=n, **kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stratify_split(df, y, cats, ratio):\n",
    "    keys = df[cats]\n",
    "    if y.dtype.name[:5] != 'float': keys = pd.concat([keys, y], axis=1)\n",
    "    keys = keys.apply(lambda x: '~'.join([str(j) for j in x.values]), axis=1)\n",
    "\n",
    "    sss = split_by_cats(train_size =1-ratio, test_size=ratio)\n",
    "    train, val = next(sss.split(df, keys))\n",
    "    x_trn, x_val = safe_indexing(df, train), safe_indexing(df, val)\n",
    "    y_trn, y_val = safe_indexing(y, train), safe_indexing(y, val)\n",
    "    return x_trn, y_trn, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_time_series(df, time_feature, ratio):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(by=time_feature, ascending=True)\n",
    "    split_id = int(df.shape*(1-ratio))\n",
    "    x_trn, y_trn = df[:split_id], y[:split_id]\n",
    "    x_val, y_val = df[split_id:], y[split_id:]\n",
    "    return x_trn, y_trn, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class split_by_cats(StratifiedShuffleSplit):\n",
    "    def _iter_indices(self, X, y, groups=None):\n",
    "        n_samples = _num_samples(X)\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n",
    "                                                  self.train_size)\n",
    "\n",
    "        if y.ndim == 2:\n",
    "            # for multi-label y, map each distinct row to a string repr\n",
    "            # using join because str(row) uses an ellipsis if len(row) > 1000\n",
    "            y = np.array([' '.join(row.astype('str')) for row in y])\n",
    "\n",
    "        classes, y_indices = np.unique(y, return_inverse=True)\n",
    "        n_classes = classes.shape[0]\n",
    "\n",
    "        class_counts = np.bincount(y_indices)\n",
    "        if np.min(class_counts) < 2:\n",
    "            print(ValueError(\"The least populated class in y has only 1\"\n",
    "                             \" member, which is too few. The minimum\"\n",
    "                             \" number of groups for any class cannot\"\n",
    "                             \" be less than 2.\"))\n",
    "\n",
    "        if n_train < n_classes:\n",
    "            print(ValueError('The train_size = %d should be greater or '\n",
    "                             'equal to the number of classes = %d' %\n",
    "                             (n_train, n_classes)))\n",
    "        if n_test < n_classes:\n",
    "            print(ValueError('The test_size = %d should be greater or '\n",
    "                             'equal to the number of classes = %d' %\n",
    "                             (n_test, n_classes)))\n",
    "\n",
    "        # Find the sorted list of instances for each class:\n",
    "        # (np.unique above performs a sort, so code is O(n logn) already)\n",
    "        class_indices = np.split(np.argsort(y_indices, kind='mergesort'),\n",
    "                                 np.cumsum(class_counts)[:-1])\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            # if there are ties in the class-counts, we want\n",
    "            # to make sure to break them anew in each iteration\n",
    "            n_i = _approximate_mode(class_counts, n_train, rng)\n",
    "            class_counts_remaining = class_counts - n_i\n",
    "            t_i = _approximate_mode(class_counts_remaining, n_test, rng)\n",
    "\n",
    "            train = []\n",
    "            test = []\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                permutation = rng.permutation(class_counts[i])\n",
    "                perm_indices_class_i = class_indices[i].take(permutation,\n",
    "                                                             mode='clip')\n",
    "\n",
    "                train.extend(perm_indices_class_i[:n_i[i]])\n",
    "                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])\n",
    "\n",
    "            train = rng.permutation(train)\n",
    "            test = rng.permutation(test)\n",
    "\n",
    "            yield train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_pre-processing.ipynb.\n",
      "Converted 03_transform.ipynb.\n",
      "Converted 04_dataset.ipynb.\n",
      "Converted 05_import.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
